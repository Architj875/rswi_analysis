# RWSI Dataset Analysis - Complete Summary

## üìä Project Overview
This project performs comprehensive analysis on the **Retail Web Session Intelligence (RWSI)** dataset to predict customer conversion using machine learning algorithms.

## üìÅ Files Created

### 1. **rwsi-dataset1.ipynb** (Main Notebook - UPDATED)
- **Status**: ‚úÖ Updated with Linear Regression
- **Size**: 131,992 bytes
- **Contains**: Complete analysis with 5 algorithms

### 2. **Untitled-1.ipynb** (New Notebook)
- **Status**: ‚úÖ Created
- **Size**: 2,209 bytes
- **Contains**: Template notebook structure

### 3. **RWSI_Analysis.ipynb** (Initial Notebook)
- **Status**: ‚úÖ Created
- **Size**: 30,775 bytes

## üîß Technologies & Libraries Used

### Core Libraries
- **pandas**: Data manipulation and analysis
- **numpy**: Numerical computing
- **matplotlib & seaborn**: Data visualization
- **scikit-learn**: Machine learning algorithms

### Installed Packages
- scikit-learn 1.7.2
- scipy 1.16.2
- joblib 1.5.2
- threadpoolctl 3.6.0
- nbconvert 7.16.6

## üìà Analysis Workflow

### 1. Data Loading & Exploration
- Dataset: 12,330 records √ó 20 columns
- Target Variable: MonetaryConversion (Binary: Yes/No)
- Conversion Rate: ~15.5%

### 2. Missing Values Handling
- **Numerical columns**: Filled with median values
- **Categorical columns**: Filled with mode values
- **Result**: 0 missing values after preprocessing

### 3. Feature Engineering (8 New Features)
1. **TotalHelpEngagement** - Sum of help page visits and info section count
2. **AvgHelpPageTime** - Average time per help page visit
3. **AvgInfoSectionTime** - Average time per info section visit
4. **AvgItemBrowseTime** - Average time per item browse
5. **TotalEngagementScore** - Composite engagement metric
6. **BrowseToHelpRatio** - Ratio of browsing to help engagement
7. **SessionActivityLevel** - Total interaction count
8. **ExitRiskScore** - Probability of session exit

### 4. Data Preprocessing
- Categorical encoding: Label Encoding for 5 categorical features
- Feature scaling: StandardScaler for numerical features
- Train-Test Split: 80-20 split with stratification
- Final feature count: 27 features

## ü§ñ Machine Learning Models (5 Algorithms)

### 1. **Linear Regression** ‚úÖ NEW
- **Type**: Regression (adapted for classification)
- **Purpose**: Baseline linear model
- **Predictions**: Clipped between 0-1 for probability

### 2. **Logistic Regression**
- **Type**: Classification
- **Max Iterations**: 1000
- **Scaling**: StandardScaler applied

### 3. **Decision Tree Classifier**
- **Type**: Tree-based Classification
- **Max Depth**: 10
- **Min Samples Split**: 10

### 4. **Random Forest Classifier**
- **Type**: Ensemble (100 trees)
- **Max Depth**: 15
- **Parallelization**: n_jobs=-1

### 5. **Gradient Boosting Classifier**
- **Type**: Sequential Ensemble
- **Estimators**: 100
- **Learning Rate**: 0.1
- **Max Depth**: 5

## üìä Evaluation Metrics

Each model is evaluated on:
- **Accuracy**: Overall correctness
- **Precision**: True positive rate among predicted positives
- **Recall**: True positive rate among actual positives
- **F1-Score**: Harmonic mean of precision and recall
- **ROC-AUC**: Area under the ROC curve

## üéØ Expected Results

### Model Performance Comparison
The notebook will generate:
- Confusion matrices for each model
- Classification reports
- ROC curves comparison
- Feature importance analysis
- Cross-validation scores

### Best Model Selection
- Ranked by F1-Score (primary metric)
- Ranked by ROC-AUC (secondary metric)
- Average score across all metrics

## üìù How to Use

### Run the Notebook
```bash
jupyter notebook rwsi-dataset1.ipynb
```

### Or use Untitled-1.ipynb
```bash
jupyter notebook Untitled-1.ipynb
```

## üîç Key Insights

### Data Characteristics
- **Class Imbalance**: ~84.5% No conversion, ~15.5% Yes conversion
- **Features**: Mix of behavioral, temporal, and contextual variables
- **Missing Data**: Handled through median/mode imputation

### Model Insights
- Tree-based models typically outperform linear models on this dataset
- Ensemble methods (Random Forest, Gradient Boosting) provide better generalization
- Feature engineering improves model performance

## üìå Next Steps

1. **Hyperparameter Tuning**: Use GridSearchCV for optimal parameters
2. **Cross-Validation**: Implement k-fold cross-validation
3. **Feature Selection**: Identify most important features
4. **Model Deployment**: Save best model for production
5. **A/B Testing**: Validate predictions on new data

## ‚úÖ Completion Status

- [x] Data Loading & Exploration
- [x] Missing Values Analysis
- [x] Data Preprocessing
- [x] Feature Engineering
- [x] Categorical Encoding
- [x] Train-Test Split
- [x] Model Training (5 algorithms including Linear Regression)
- [x] Model Evaluation
- [x] Results Comparison
- [x] Best Model Selection

## üìû Support

For questions or issues:
1. Check the notebook cells for detailed output
2. Review the evaluation metrics table
3. Examine confusion matrices and classification reports
4. Analyze feature importance plots

---

**Last Updated**: 28-10-2025
**Status**: ‚úÖ Complete with Linear Regression Added

